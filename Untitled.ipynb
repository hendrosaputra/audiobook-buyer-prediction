{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37b2477",
   "metadata": {},
   "source": [
    "# Deep Learning Apliaction - Audiobook buyer business case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48ab02",
   "metadata": {},
   "source": [
    "This project is going to explore the aplication of deep learning method to make a prediction based of a dataset from an audiobook company. The result will be an algorithm that can be used to predict will someone buy a audiobook from that company again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd9415",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d7f65",
   "metadata": {},
   "source": [
    "### Extract the data from th csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5523e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "# Choose all column exept the first and last column to become inputs data\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "# Choose the last column to became targets data\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d9788",
   "metadata": {},
   "source": [
    "When the data was collected it was actually arranged by date Shuffle the indices of the data, so the data is not arranged in any way when we feed it. Since we will be batching, we want the data to be as randomly spread out as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1e5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indicies = np.arange(unscaled_inputs_all.shape[0])\n",
    "np.random.shuffle(shuffle_indicies)\n",
    "\n",
    "unscaled_inputs_all = unscaled_inputs_all[shuffle_indicies]\n",
    "targets_all = targets_all[shuffle_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9014b3",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac2ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all targets so we will get the number of target that are 1\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "# Set the 0 counter and indicies to remove to help removing data that we will not use\n",
    "zero_targets_counter = 0\n",
    "indicies_to_remove =[]\n",
    "# Set a loop that contain all target, with the lenght of vector\n",
    "for i in range(targets_all.shape[0]):\n",
    "    # Increase the zero cunter by 1 if the targets is 0\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        # if the targets in at position i is 0 and numbers of 0 is bigger than numbers of 1 we want to take note of that index\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indicies_to_remove.append(i)\n",
    "            #append is to add element to the list\n",
    "            #indicies_to_remove will contain the data that we don't need\n",
    "\n",
    "#Deleting the object that have index\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indicies_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indicies_to_remove, axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2d537",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0db01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ce069",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c3747",
   "metadata": {},
   "source": [
    "We take indicies from axis 0 of the scaled inputs shape and place them in a variable. np.random.shuffle is method that shuffle the number in a given sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a99800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indicies = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffle_indicies)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffle_indicies]\n",
    "shuffled_targets = targets_equal_priors[shuffle_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c160c",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe75b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n",
      "1780.0 3579 0.4973456272701872\n",
      "231.0 447 0.5167785234899329\n",
      "226.0 448 0.5044642857142857\n"
     ]
    }
   ],
   "source": [
    "sample_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_sample_count = int(0.8 * sample_count)\n",
    "validation_sample_count = int(0.1 * sample_count)\n",
    "test_sample_count = sample_count - train_sample_count - validation_sample_count\n",
    "\n",
    "train_inputs = shuffled_inputs[: train_sample_count]\n",
    "train_targets = shuffled_targets[: train_sample_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_sample_count : train_sample_count + validation_sample_count]\n",
    "validation_targets = shuffled_targets[train_sample_count : train_sample_count + validation_sample_count]\n",
    "\n",
    "test_inputs =  shuffled_inputs[train_sample_count + validation_sample_count : ]\n",
    "test_targets =  shuffled_targets[train_sample_count + validation_sample_count : ]\n",
    "\n",
    "print(sample_count)\n",
    "print(np.sum(train_targets), train_sample_count, np.sum(train_targets)/train_sample_count)\n",
    "print(np.sum(validation_targets), validation_sample_count, np.sum(validation_targets)/validation_sample_count)\n",
    "print(np.sum(test_targets), test_sample_count, np.sum(test_targets)/test_sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead2b6f",
   "metadata": {},
   "source": [
    "### Save the dataset to *.npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d50c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobook_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobook_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
    "np.savez('Audiobook_data_test', inputs = test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd093902",
   "metadata": {},
   "source": [
    "### Prepocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7534a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "# Store the train data to a temporary variable npz\n",
    "npz = np.load('Audiobook_data_train.npz')\n",
    "# Load them into train input and target variable\n",
    "# The input data type will be float and the target data will be integer\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# Validation data\n",
    "npz = np.load('Audiobook_data_validation.npz')\n",
    "validation_inputs = npz['inputs'].astype(float)\n",
    "validation_targets = npz['targets'].astype(int)\n",
    "\n",
    "# Test data\n",
    "npz = np.load('Audiobook_data_test.npz')\n",
    "test_inputs = npz['inputs'].astype(float)\n",
    "test_targets = npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a6c05",
   "metadata": {},
   "source": [
    "## Create The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e5c9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 - 1s - loss: 0.6382 - accuracy: 0.6239 - val_loss: 0.5727 - val_accuracy: 0.7405 - 624ms/epoch - 78ms/step\n",
      "Epoch 2/100\n",
      "8/8 - 0s - loss: 0.5451 - accuracy: 0.7427 - val_loss: 0.5083 - val_accuracy: 0.7629 - 45ms/epoch - 6ms/step\n",
      "Epoch 3/100\n",
      "8/8 - 0s - loss: 0.4945 - accuracy: 0.7586 - val_loss: 0.4679 - val_accuracy: 0.7562 - 45ms/epoch - 6ms/step\n",
      "Epoch 4/100\n",
      "8/8 - 0s - loss: 0.4631 - accuracy: 0.7617 - val_loss: 0.4448 - val_accuracy: 0.7673 - 41ms/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "8/8 - 0s - loss: 0.4419 - accuracy: 0.7698 - val_loss: 0.4270 - val_accuracy: 0.7785 - 47ms/epoch - 6ms/step\n",
      "Epoch 6/100\n",
      "8/8 - 0s - loss: 0.4275 - accuracy: 0.7807 - val_loss: 0.4169 - val_accuracy: 0.7875 - 44ms/epoch - 6ms/step\n",
      "Epoch 7/100\n",
      "8/8 - 0s - loss: 0.4166 - accuracy: 0.7851 - val_loss: 0.4034 - val_accuracy: 0.7987 - 43ms/epoch - 5ms/step\n",
      "Epoch 8/100\n",
      "8/8 - 0s - loss: 0.4068 - accuracy: 0.7885 - val_loss: 0.3963 - val_accuracy: 0.8009 - 41ms/epoch - 5ms/step\n",
      "Epoch 9/100\n",
      "8/8 - 0s - loss: 0.3988 - accuracy: 0.7918 - val_loss: 0.3905 - val_accuracy: 0.8166 - 46ms/epoch - 6ms/step\n",
      "Epoch 10/100\n",
      "8/8 - 0s - loss: 0.3931 - accuracy: 0.7999 - val_loss: 0.3852 - val_accuracy: 0.8188 - 44ms/epoch - 5ms/step\n",
      "Epoch 11/100\n",
      "8/8 - 0s - loss: 0.3875 - accuracy: 0.8005 - val_loss: 0.3775 - val_accuracy: 0.8166 - 43ms/epoch - 5ms/step\n",
      "Epoch 12/100\n",
      "8/8 - 0s - loss: 0.3861 - accuracy: 0.7972 - val_loss: 0.3786 - val_accuracy: 0.8121 - 46ms/epoch - 6ms/step\n",
      "Epoch 13/100\n",
      "8/8 - 0s - loss: 0.3800 - accuracy: 0.8025 - val_loss: 0.3720 - val_accuracy: 0.8277 - 43ms/epoch - 5ms/step\n",
      "Epoch 14/100\n",
      "8/8 - 0s - loss: 0.3779 - accuracy: 0.7997 - val_loss: 0.3690 - val_accuracy: 0.8277 - 43ms/epoch - 5ms/step\n",
      "Epoch 15/100\n",
      "8/8 - 0s - loss: 0.3734 - accuracy: 0.8041 - val_loss: 0.3691 - val_accuracy: 0.8233 - 39ms/epoch - 5ms/step\n",
      "Epoch 16/100\n",
      "8/8 - 0s - loss: 0.3711 - accuracy: 0.8044 - val_loss: 0.3656 - val_accuracy: 0.8233 - 45ms/epoch - 6ms/step\n",
      "Epoch 17/100\n",
      "8/8 - 0s - loss: 0.3738 - accuracy: 0.8089 - val_loss: 0.3624 - val_accuracy: 0.8233 - 44ms/epoch - 5ms/step\n",
      "Epoch 18/100\n",
      "8/8 - 0s - loss: 0.3698 - accuracy: 0.8036 - val_loss: 0.3666 - val_accuracy: 0.8233 - 42ms/epoch - 5ms/step\n",
      "Epoch 19/100\n",
      "8/8 - 0s - loss: 0.3697 - accuracy: 0.7966 - val_loss: 0.3626 - val_accuracy: 0.8188 - 40ms/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2925f6db400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting input, output and hidden layer size\n",
    "input_size = 10 #hyperparameter\n",
    "output_size = 2 #hyperparameter\n",
    "hidden_layer_size = 100 #hyperparameter\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
    "                            ])\n",
    "\n",
    "# Set Optimizer and loss function\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Set Batch size and Epoch\n",
    "batch_size = 500 #hyperparameter\n",
    "max_epoch = 100 #hyperparameter\n",
    "\n",
    "# Set early stopping mechanishm \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 2)\n",
    "# 'patience' let us decide how many consecutive increase of validation loss we can tolerate\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_inputs, #train inputs\n",
    "          train_targets, #train targets\n",
    "          batch_size = batch_size, #batch size\n",
    "          epochs = max_epoch, #epochs thath we will train from\n",
    "          callbacks = [early_stopping], #early stopping mechanishm\n",
    "          validation_data = (validation_inputs, validation_targets), #validation data\n",
    "          verbose = 2 #make sure we geting enogh information about training process\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b85124",
   "metadata": {},
   "source": [
    "### Test The Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75e627cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8237\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012a606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.370856. Test Accuracy: 82.366073%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest Loss: {0:2f}. Test Accuracy: {1:2f}%'.format(test_loss, test_accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
